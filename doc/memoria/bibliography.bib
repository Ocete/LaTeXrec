@misc{deng2017imagetomarkup,
      title={Image-to-Markup Generation with Coarse-to-Fine Attention}, 
      author={Yuntian Deng and Anssi Kanervisto and Jeffrey Ling and Alexander M. Rush},
      year={2017},
      eprint={1609.04938},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{vaswani2017attention,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{smith2018superconvergence,
      title={Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates}, 
      author={Leslie N. Smith and Nicholay Topin},
      year={2018},
      eprint={1708.07120},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{rff,
author = {Rahimi, Ali and Recht, Benjamin},
title = {Random Features for Large-Scale Kernel Machines},
year = {2007},
isbn = {9781605603520},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
booktitle = {Proceedings of the 20th International Conference on Neural Information Processing Systems},
pages = {1177â€“1184},
numpages = {8},
location = {Vancouver, British Columbia, Canada},
series = {NIPS'07}
}

@dataset{kanervisto_anssi_2016_56198,
  author       = {Kanervisto, Anssi},
  title        = {im2latex-100k ,  arXiv:1609.04938},
  month        = jun,
  year         = 2016,
  publisher    = {Zenodo},
  doi          = {10.5281/zenodo.56198},
  url          = {https://doi.org/10.5281/zenodo.56198}
}

@dataset{im2latex_170k,
  author       = {Blake Vente, Alex Taradachuk},
  title        = {im2latex-170k},
  year         = 2020,
  url          = {https://www.kaggle.com/rvente/im2latex170k/metadata}
}

@online{bloem_transformers,
	title = {Transformers from scratch},
	url = {http://peterbloem.nl/blog/transformers},
	urldate = {2020-12-21},
	year = {2019},
	author = {Peter Bloem}
}

@online{tensorflow_transformer,
	title = {Transformer model for language understanding},
	url = {https://www.tensorflow.org/tutorials/text/transformer},
	author = {{Tensorflow team}},
	titleaddon = {{TensorFlow}},
	urldate = {2020-12-21},
	year = {2020},
	langid = {english},
}

@online{tokenizer,
	title = {Tensorflow Keras Tokenizer},
	url = {https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer},
	author = {{Tensorflow team}},
	titleaddon = {{TensorFlow}},
	urldate = {2020-12-21},
	year = {2020},
	langid = {english},
}

@online{tf_dataset,
	title = {Tensorflow \texttt{Dataset} class},
	url = {https://www.tensorflow.org/api_docs/python/tf/data/Dataset},
	author = {{Tensorflow team}},
	titleaddon = {{TensorFlow}},
	urldate = {2021-01-28},
	year = {2020},
	langid = {english},
}

@misc{tay2020efficient,
      title={Efficient Transformers: A Survey}, 
      author={Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
      year={2020},
      eprint={2009.06732},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{choromanski_rethinking_2020,
	title = {Rethinking Attention with Performers},
	url = {http://arxiv.org/abs/2009.14794},
	journaltitle = {{arXiv}:2009.14794 [cs, stat]},
	author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
	urldate = {2021-01-13},
	date = {2020-09-30},
	eprinttype = {arxiv},
	eprint = {2009.14794},
}